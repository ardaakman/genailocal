{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "class GPTModel:\n",
    "    def __init__(self):\n",
    "        self.model = \"gpt-4o\"\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    # Function to encode the image\n",
    "    def encode_image(self, image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    def forward(self, image, prompt):\n",
    "        formatted_image = image if image.startswith(\"http\") else f\"data:image/jpeg;base64,{self.encode_image(image)}\"\n",
    "        image_dict = {\"url\": formatted_image}\n",
    "        prompt_dict = {\"type\": \"text\", \"text\": prompt}\n",
    "        image_dict = {\"type\": \"image_url\", \"image_url\": image_dict}\n",
    "        user_content = [prompt_dict, image_dict]\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=messages)\n",
    "        return response.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the events from your calendar for September 7th to September 10th, 2024:\n",
      "\n",
      "### Saturday, September 7th, 2024\n",
      "- **Event:** Canâ€™t Stop! (Rush/Mmmpie Knar En)\n",
      "  - **Start Time:** 1 AM\n",
      "  - **End Time:** 2 AM\n",
      "  \n",
      "- **Event:** Weekly Reflection\n",
      "  - **Start Time:** 4 AM\n",
      "  - **End Time:** 4:45 AM\n",
      "\n",
      "### Sunday, September 8th, 2024\n",
      "There are no events on this day.\n",
      "\n",
      "### Monday, September 9th, 2024\n",
      "- **Event:** Yamusan\n",
      "  - **Start Time:** 8 AM\n",
      "  - **End Time:** 12 PM\n",
      "  \n",
      "- **Event:** AI Goes Local\n",
      "  - **Start Time:** 11 AM\n",
      "  - **End Time:** 8 PM\n",
      "\n",
      "- **Event:** Calvin\n",
      "  - **Start Time:** 3:30 PM\n",
      "  - **End Time:** 4:30 PM\n",
      "  - **Location:** 1600 Mission St\n",
      "\n",
      "- **Event:** SF Startup Series\n",
      "  - **Start Time:** 5 PM\n",
      "  - **End Time:** 6 PM\n",
      "  - **Location:** 441 Oak St\n",
      "\n",
      "- **Event:** Arya\n",
      "  - **Start Time:** 7 PM\n",
      "  - **End Time:** 8 PM\n",
      "\n",
      "- **Event:** Housewarming Dinner\n",
      "  - **Start Time:** 7:30 PM\n",
      "  - **End Time:** 9:30 PM\n",
      "\n",
      "### Tuesday, September 10th, 2024\n",
      "- **Event:** Eif w/ Deniz\n",
      "  - **Start Time:** 8 AM\n",
      "  - **End Time:** 8:45 AM\n"
     ]
    }
   ],
   "source": [
    "print(gpt.forward(\"images/calendar.png\", \"Look at my calendar and output all the events that I have, when they start, and when they end.\").message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Main Evaluation Results\n",
      "\n",
      "### Metrics:\n",
      "- **Avg Score:** The average score on all VLM Benchmarks (normalized to 0 - 100, the higher the better).\n",
      "- **Avg Rank:** The average rank on all VLM Benchmarks (the lower the better).\n",
      "- **Avg Score & Rank** are calculated based on the selected benchmark. When results for some selected benchmarks are missing, Avg Score / Rank will be None.\n",
      "- By default, the overall evaluation results are based on 8 VLM benchmarks, sorted by the descending order of Avg Score.\n",
      "  - The following datasets are included in the main results: MMBench_V11, MMStar, MMMU_VAL, MathVista, OCRBench, AID2, HallusionBench, MMVet.\n",
      "  - Detailed evaluation results for each dataset (included or not included in main) are provided in the consequent tabs.\n",
      "\n",
      "### Evaluation Dimension\n",
      "\n",
      "| Avg Score | Avg Rank | MMBench_V11 | MMStar | MME | MMMU_VAL | MathVista | OCRBench | AID2 | HallusionBench | SEEDBench_IMG | LLaVA-Bench | CCBench | RealWorldQA | POPE | ScienceQA_TEST | SEEDBench2_Plus | MMT-Bench_VAL | BLINK |\n",
      "\n",
      "### Model Size:\n",
      "- <4B\n",
      "- 4B-10B\n",
      "- 10B-20B\n",
      "- 20B-40B\n",
      "- >40B\n",
      "- Unknown\n",
      "\n",
      "### Model Type:\n",
      "- API\n",
      "- OpenSource\n",
      "- Proprietary\n",
      "\n",
      "### Table of Results\n",
      "\n",
      "| Rank | Method                    | Param (B) | Language Model    | Vision Model         | Avg Score | Avg Rank | MMBench_V11 | MMStar | MMMU_VAL | MathVista | OCR |\n",
      "|------|---------------------------|-----------|--------------------|----------------------|-----------|----------|-------------|--------|-----------|------------|-----|\n",
      "| 1    | MiniCPM-V-2.6             | 8         | Qwen2-7B           | SigLIP-400M          | 65.2      | 13.62    | 78          | 57.5   | 49.8      | 60.6       | 852 |\n",
      "| 2    | InternVL2-8B              | 8         | InternLM2-5.7B     | InternViT-300M       | 64.1      | 15       | 79.4        | 61.5   | 51.2      | 58.3       | 794 |\n",
      "| 5    | InternVL2-4B              | 4         | Phi-3              | InternViT-300M       | 60.6      | 25.5     | 73.6        | 53.9   | 48.3      | 58.1       | 784 |\n",
      "| 12   | InternVL2-2B              | 2         | InternLM2-1.8B     | InternViT-300M       | 54        | 49.5     | 69.6        | 49.8   | 36.3      | 46         | 781 |\n",
      "| 6    | GLM-4v-9B                 | 9         | GLM-4-9B           | EVA-02-5B            | 59.1      | 31.75    | 67.9        | 54.8   | 46.9      | 51.1       | 776 |\n",
      "| 26   | InternVL2-1B              | 1         | Qwen2-0.5B         | InternViT-300M       | 48.3      | 50       | 59.7        | 45.6   | 36.7      | 39.4       | 755 |\n",
      "| 3    | Ovisi.5-Llama3-8B         | 8         | Llama-3-8B-Instruct| SigLIP-400M          | 62.2      | 20       | 76.6        | 57.7   | 48.3      | 63         | 744 |\n",
      "| 7    | MiniCPM-Llama3-V2.5-8     | 8         | Llama-3-8B-Instruct| SigLIP-400M          | 58.8      | 30       | 72          | 51.8   | 45.8      | 54.3       | 725 |\n",
      "| 4    | InternLM-XComposer2.5-8   | 8         | InternLM2-7B       | CLIP ViT-L/14        | 61        | 23.25    | 79.4        | 59.9   | 42.9      | 63.7       | 686 |\n",
      "| 8    | InternLM-XComposer2-4.7   | 7         | InternLM2-7B       | CLIP ViT-L/14        | 58.3      | 31.12    | 76.5        | 55.3   | 39.7      | 59.4       | 675 |\n",
      "| 21   | Mini-InternVL2-Chat-2B    | 2         | InternLM2-1.8B     | InternViT-300M       | 51.6      | 41       | 65.2        | 49.2   | 46.7      | 41.3       | 652 |\n",
      "| 22   | Mini-InternVL2-Chat-4B    | 4         | Phi-3              | InternViT-300M       | 56.2      | 37.25    | 69.7        | 53.1   | 45.1      | 54.6       | 639 |\n",
      "| 15   | GLM-2-Wisdom              | 6.5       | GLM-6/10B          | InternViT-300M       | 55.8      | 45.72    | 70.2        | 49.2   | 48.6      | 52.1       | 627 |\n",
      "\n",
      "### Notes\n",
      "- **Citation:** Not provided.\n",
      "\n",
      "**Use via API:** Built with Gradio.\n"
     ]
    }
   ],
   "source": [
    "print(gpt.forward(\"images/evals.png\", \"Parse all the information on the string, output in terms of text or a markdown format. Do not truncate information.\").message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screenshot shows a dashboard for visualizing machine learning experiments in a project titled \"**hw2p2**\" within the workspace of a user called \"Denizbirlikci\" on a platform that appears to be Weights & Biases (W&B).\n",
      "\n",
      "Key elements of the screenshot:\n",
      "\n",
      "1. **Warning Banner**: The user has exceeded 100% of their tracking hours and is prompted to upgrade their plan to continue using W&B without interruption.\n",
      "   \n",
      "2. **Project Header**: The project is named \"raj_deniz_josh_simrit,\" and the specific sub-project or notebook in use is \"hw2p2.\"\n",
      "\n",
      "3. **Workspace Overview**: \n",
      "\n",
      "    - The workspace is showing multiple runs (62 total in this example) with a table of the different runs on the left side, each with unique names (e.g., \"raj-rewritten-convnxt,\" \"raj-adam-with-warmup,\" \"deniz:viShNet:AdamW-RETRY\").\n",
      "    - Various icons indicate the status, tags, and other attributes of these runs.\n",
      "\n",
      "4. **Charts/Visualizations**: Six main charts illustrate the results of different runs:\n",
      "   \n",
      "    - **Train Accuracy (train_Acc)**\n",
      "    - **Train Loss (train_loss)**\n",
      "    - **Validation Accuracy (validation_Acc)**\n",
      "    - **Validation Loss (validation_loss)**\n",
      "    - **Learning Rate (learning_Rate)**\n",
      "    - **System Metrics**: GPU Power Usage, GPU Power Usage (%), and GPU Memory Allocated (%)\n",
      "\n",
      "5. **Run Comparisons**: The charts compare various runs using different hyperparameters or experimental setups. Comparison runs are color-coded for clarity.\n",
      "\n",
      "6. **Interface Elements**: Standard interface elements for W&B like the option to add panels or filters, and tabs for Overview, Runs, Sweeps, Reports, Artifacts, etc.\n",
      "\n",
      "At the bottom, there are various applications open on the macOS dock, including iMessage, Mail, Finder, Safari, GitHub Desktop, Terminal, and several others.\n",
      "\n",
      "The dashboard is designed to help users track and visualize the performance of their machine learning experiments over time, making it easier to compare different training configurations and select the optimal one.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You're a helpful assistant to take a screenshot of a UI and explain everything going on in the interface. \n",
    "\n",
    "For this image, explain each graph you see. Write a few details on how runs have formulated. \n",
    "\n",
    "Imagine that you're explaining this to someone that will never see the image.\n",
    "\n",
    "Explain each graph and how each run's performance on each graph looks like. \n",
    "\"\"\"\n",
    "\n",
    "print(gpt.forward(\"images/wandb.png\", \"\").message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt.forward(\"images/calendar.png\", \"Parse all the information on the string, output in terms of text or a markdown format. Do not truncate information.\").message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='# Main Evaluation Results\\n\\n## Metrics:\\n- **Avg Score**: The average score on all VLM Benchmarks (normalized to 0 - 100, the higher the better).\\n- **Avg Rank**: The average rank on all VLM Benchmarks (the lower the better).\\n- **Avg Score & Rank** are calculated based on selected benchmark. When results for some selected benchmarks are missing, Avg Score / Rank will be None!!\\n\\nBy default, we present the overall evaluation results based on 8 VLM benchmarks, sorted by the descending order of Avg Score.\\n- The following datasets are included in the main results: MMBench_V11, MMStar, MME, MMU_VAL, MathVista, OCRBench, AID2, HallusionBench, SEEDBench, MMVet.\\n- Detailed evaluation results for each dataset (included or not included in main) are provided in the consequent tabs.\\n\\n## Evaluation Dimension\\n- Avg Score\\n- Avg Rank\\n- MMBench_V11\\n- MMStar\\n- MME\\n- MMU_VAL\\n- MathVista\\n- OCRBench\\n- AID2\\n- HallusionBench\\n- SEEDBench_Plus\\n- MMT-Bench_VAL\\n- BLINK\\n\\n## Model Size\\n- <4B\\n- 4B-10B\\n- 10B-20B\\n- 20B-40B\\n- >40B\\n- Unknown\\n\\n## Rankings\\n| Rank | Method               | Param (B) | Language Model     | Vision Model      | Avg Score | Avg Rank | MMBench_V11 | MMStar | MMU_VAL | MathVista | OCR |\\n|------|----------------------|-----------|---------------------|--------------------|-----------|----------|--------------|--------|---------|-----------|-----|\\n| 1    | MiniCPM-V.2.6       | 8         | Qwen2-7B            | SigLIP-400M        | 65.2      | 13.62    | 78           | 57.5   | 49.8    | 60.6      | 852 |\\n| 2    | InternVL-2-8        | 8         | InternLM2.5-7B      | InternViT-300M     | 64.1      | 15       | 79.4         | 61.5   | 51.2    | 58.3      | 794 |\\n| 3    | InternVL2-4B        | 4         | Phi-3               | InternLM2-3B       | 60.6      | 25.5     | 73.6         | 53.9   | 48.3    | 58.1      | 784 |\\n| 4    | InternVL2-2B        | 2         | InternLM2-1.8B      | InternViT-300M     | 54        | 49.5     | 69.6         | 49.8   | 36.3    | 46       | 781 |\\n| 5    | GLM-4v-9B           | 9         | GLM-4.0             | EVA-02-5B          | 59.1      | 31.75    | 67.9         | 54.8   | 46.9    | 51.1      | 776 |\\n| 6    | InternVL2-1B        | 1         | Qwen2-0.5B          | InternViT-300M     | 48.3      | 69.38    | 59.7         | 45.6   | 36.7    | 39.4      | 755 |\\n| 7    | Ovis1.5-Llama3-8B   | 8         | Llama-3-8B-Insturct | SigLIP-400M        | 62.2      | 20       | 76.6         | 57.3   | 48.3    | 63       | 744 |\\n| 8    | MiniCPM-Llama3-V2.5 | 8         | Llama-3-8B-Insturct | SigLIP-400M        | 62.2      | 30       | 72           | 51.8   | 54.3    | 725       | 725 |\\n| 9    | InternLM-XComposer2.5| 8         | InternLM2-7B        | CLIP ViT-L/14      | 31.12     | 76.5     | 55.3         | 39.7   | 59.4    | 675       | 675 |\\n| 10   | InternLM-XComposer2-4| 7         | InternLM2-7B        | CLIP ViT-L/14      | 31.12     | 76.5     | 65.2         | 46.7   | 37.4    | 41.3      | 652 |\\n| 11   | InternVL-Chat-2     | 2         | InternML-1.8B       | InternViT-300M     | 37.25     | 69.7     | 53.1         | 45.1   | 54.6    | 639       | 639 |\\n| 12   | Mini-InternVL-Chat-4| 4         | Phi-3               | InternViT-300M     | 65.2      | 48.17    | 67.3         | 51.3   | 45.1    | 627       | 627 |\\n\\n## Use via API\\n- Built with Gradio ðŸ¦™', refusal=None, role='assistant', function_call=None, tool_calls=None))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.forward(\"images/evals.png\", \"Parse all the information on the string, output in terms of text or a markdown format. Do not truncate information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s a parsed representation of the information presented in the image:\n",
      "\n",
      "## Main Evaluation Results\n",
      "\n",
      "### Metrics\n",
      "- **Avg Score**: The average score on all VLM Benchmarks (normalized to 0 - 100, the higher the better).\n",
      "- **Avg Rank**: The average rank on all VLM Benchmarks (the lower the better).\n",
      "- **Avg Score & Rank**: Calculated based on selected benchmarks. If results for some benchmarks are missing, Avg Score/Rank will be `None`.\n",
      "\n",
      "### Evaluation Dimension\n",
      "- The results are organized by descending order of Avg Score.\n",
      "- The following datasets are included:\n",
      "  - MMBench_V11\n",
      "  - MMStar\n",
      "  - MME\n",
      "  - MMU_VAL\n",
      "  - MathVista\n",
      "  - OCRBench\n",
      "  - AID2\n",
      "  - HallusionBench\n",
      "  - SEEDBench_Plus\n",
      "  - MMT-Bench_VAL\n",
      "  - BLINK\n",
      "\n",
      "### Model Size\n",
      "- Options are available for <4B, 4B-10B, 10B-20B, 20B-40B, >40B, Unknown.\n",
      "\n",
      "### Table: Model Performance\n",
      "| Rank | Method               | Params (B) | Language Model     | Vision Model       | Avg Score | Avg Rank | MMBench_V11 | MMStar | MMU_VAL | MathVista | OCR |\n",
      "|------|---------------------|------------|---------------------|---------------------|-----------|----------|-------------|--------|---------|-----------|-----|\n",
      "| 1    | MiniCPM-V.2.6      | 8          | Qwen2-7B            | SigLIP-400M         | 65.2      | 13.62    | 78          | 57.5   | 49.8    | 60.6      | 852 |\n",
      "| 2    | InternVL-2B-8      | 8          | InternLM2-5-7B      | InternViT-300M      | 64.1      | 15       | 79.4        | 61.5   | 51.2    | 58.3      | 794 |\n",
      "| 3    | InternVL-2-4B      | 4          | Phi-3               | InternViT-300M      | 60.6      | 25.5     | 73.6        | 53.9   | 48.3    | 58.1      | 784 |\n",
      "| 4    | InternVL-2-2B      | 2          | InternLM2-1.8B      | InternViT-300M      | 54        | 49.5     | 69.6        | 49.8   | 36.3    | 46       | 781 |\n",
      "| 5    | GLM-4v-9B          | 9          | GLM-4.9             | EVA-02-5B           | 59.1      | 31.75    | 67.9        | 54.8   | 46.9    | 51.1      | 776 |\n",
      "| 26   | InternVL-1B        | -          | Qwen2-0.5B          | -                   | 48.3      | 54.5     | 67.9        | 59.7   | 36.7    | 39.4      | 755 |\n",
      "| ...  | ...                 | ...        | ...                 | ...                 | ...       | ...      | ...         | ...    | ...     | ...       | ... |\n",
      "\n",
      "### Model Type\n",
      "- **API**\n",
      "- **OpenSource**\n",
      "- **Proprietary**\n",
      "\n",
      "### Citation\n",
      "Use via API.  \n",
      "Built with Gradio.\n"
     ]
    }
   ],
   "source": [
    "print(_12.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
